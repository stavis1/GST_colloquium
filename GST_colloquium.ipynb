{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stavis1/GST_colloquium/blob/main/GST_colloquium.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GST Colloquium on Python coding\n",
        "\n",
        "\n",
        "*   random grab-bag of tips and tricks\n",
        "*   hyper-optimization is not encouraged in Python but these techniques can provide large performace gains for little-to-no extra work\n",
        "*   In many cases these optimizations don't matter much and we've chosen small examples for practicality but when working on large datasets they can provide major practical improvements\n",
        "\n"
      ],
      "metadata": {
        "id": "dvSqLtCbA3vV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TESTING ONLY\n",
        "#this allows us to interact with google drive from within python\n",
        "from google.colab import drive\n",
        "#we mount our google drive which means that we make it acessable as a hard drive\n",
        "#in our file system under the path '/content/drive'\n",
        "drive.mount('/content/drive')\n",
        "!cp /content/drive/MyDrive/GST_colloquium/* ."
      ],
      "metadata": {
        "id": "sC98RVF-BDsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7c53e22-7a0a-4c26-90e6-0e276f4deede"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#colab comes with most common packages installed already but we want to use a\n",
        "#few uncommon ones so we install these ourselves.\n",
        "#The \"!\" tells jupyter to pass the line to the bash shell, this allows us to run\n",
        "#command line tools like pip from within jupyter notebook.\n",
        "!pip install brain-isotopic-distribution\n",
        "!pip install multiprocesspandas\n",
        "!pip install xlsx2csv"
      ],
      "metadata": {
        "id": "gTQu0A6eB3qD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#it is generally recommended best practice to import the packages that come with\n",
        "#python first before importing third party packages.\n",
        "from collections import defaultdict\n",
        "from functools import cache\n",
        "from multiprocessing import Pool\n",
        "import time\n",
        "import os\n",
        "import warnings\n",
        "from io import StringIO\n",
        "\n",
        "from brainpy import isotopic_variants\n",
        "from numba import jit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import multiprocesspandas\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import zscore\n",
        "from sortedcontainers import SortedList\n",
        "from xlsx2csv import Xlsx2csv"
      ],
      "metadata": {
        "id": "_M3C91KvBy-K"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This defines a function decorator, a concept which is used in Â§Multiprocessing: Numba\n",
        "#It times a function call and prints the elapsed time while suppressing warnings\n",
        "def time_this(func):\n",
        "  #the way this works is that time_this() creates and returns a new function\n",
        "  #that replaces whatever funciton follows @time_this. But, because\n",
        "  def inner(*args, **kwargs):\n",
        "    #this context manager suppresses all warnings. DO NOT DO THIS NORMALLY\n",
        "    #We have extensively tested this code and data and know that the warnings\n",
        "    #can be safely ignored for this tutorial but that is not true for most code\n",
        "    with warnings.filterwarnings('ignore'):\n",
        "      start_time = time.process_time()\n",
        "      result = func(*args, **kwargs)\n",
        "      elapsed_time = time.process_time() - start_time\n",
        "    print(f'Running {func.__name__} took {elapsed_time} seconds.')\n",
        "    return result\n",
        "  return inner\n",
        "\n",
        "#A test version without the warning suppression\n",
        "def time_this(func):\n",
        "  def inner(*args, **kwargs):\n",
        "    start_time = time.process_time()\n",
        "    result = func(*args, **kwargs)\n",
        "    elapsed_time = time.process_time() - start_time\n",
        "    print(f'Running {func.__name__} took {elapsed_time} seconds.')\n",
        "    return result\n",
        "  return inner"
      ],
      "metadata": {
        "id": "1SOGhargJ13-"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Structures\n",
        "\n",
        "The way data are stored in memory can have an enormous effect on the speed with which they can be processed for particular tasks. Choosing the correct data structure can be the difference between a runtime of hours or minutes in some situations."
      ],
      "metadata": {
        "id": "JJvq3pfCHxEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hashmaps\n",
        "\n",
        "Hashmaps are data structures that store key value pairs in a way that allows you to very quickly look up a value based on a key. Dictionaries in python are hashmaps which can store any object as a value and can use any hashable object as a key. Sets are specialized hashmaps which only store true or false as values. These can be used to efficiently look up whether or not a key is in the set.\n",
        "\n",
        "A hash is a function that maps objects of arbitrary size and complexity to a finite range of numbers. In the context of a hashmap data structure the finite range represents locations in memory and the hash function calculates where in memory a value is stored based on its key which means that the time it takes to find a value is only dependent on the time it takes to calculate the hash of a key. This contrasts with looking up values in a list which requires iterating over each item in the list until you find what you're looking for, an operation that becomes more expensive on average as the list grows in size.\n",
        "\n",
        "\n",
        "<img align=\"left\" src=\"https://github.com/m-j-keller/GST_colloquium/blob/main/Hashmapfigure-margin.png?raw=true\" alt=\"Figure 1. Hashmap Data Call\" title=\"Figure 1: Accessing Data in a Hashmap\" width=\"440\" hspace=\"20\" vspace=\"10\"> The tradeoff that you have to make with hashmaps is that there are some cases in which two keys have the same hash so their values must be stored in a list and iterated over for lookup. To minimize this problem the range of memory locations needs to be significantly larger than the number of values stored. This means that hash maps will generally be larger in memory than more compact data structures like lists. This is not as much of a problem as it first appears because  Python does not store the actual value at the memory locaiton calculated by the hash function. Instead the memory location it calculates is the index of a list that stores memory addresses which point to where the values are actually stored. This means that each block of memory indexed by the hashmap can be very small while still being able to point to values of arbitrary size. Therefore the memory overhead is only likely to be siginficant if you're storing a large number of small things.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**TLDR:** Storing data in hashmaps (like a python dictionary) can significantly improve data access speed, thereby speeding up the whole script.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DBtrYU4mIrTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Search trees"
      ],
      "metadata": {
        "id": "1x8476GZIz9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 1: fuzzy matching"
      ],
      "metadata": {
        "id": "_yuGS-suI6Wv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 2: file parsing"
      ],
      "metadata": {
        "id": "wTzgj_HwJOyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 3: isotope abundance calculations"
      ],
      "metadata": {
        "id": "8xdqdzaoJan6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pandas and Numpy"
      ],
      "metadata": {
        "id": "fmc_HwHBHzip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Faster .xlsx import\n",
        "Large .xlsx files can be slow to import with pandas. This is because they are actually zipped folders of mostly XML files which need to be decompressed before going through the somewhat inefficient process of XML parsing. This setup gives excel the flexability to store images, graphs, formatting, and functions but, if all you want is a table of data, this flexability comes at significant performance costs. Thus it is generally preferable to store tabular data as a text file (.csv or .tsv) unless you absolutely need to store these   extra types of information.\n",
        "\n",
        "If you have no choice but to use large .xlsx files there is still a faster way to import the data as a pandas dataframe. The xlsx2csv package is sigificantly faster at parsing .xlsx files than pandas. So, what we can do is parse the .xlsx file to an in-memory .csv (meaning that the file is stored only in RAM and not written to our hard drive) then use the very efficient pandas .csv parser to get the data into a dataframe. This trick comes from [stackoverflow](https://stackoverflow.com/questions/28766133/faster-way-to-read-excel-files-to-pandas-dataframe#comment136215400_28769537)."
      ],
      "metadata": {
        "id": "C6KC1t-RN9To"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@time_this\n",
        "def fast_import(file_path):\n",
        "    buffer = StringIO()\n",
        "    Xlsx2csv(file_path, outputencoding=\"utf-8\").convert(buffer,sheetid=0)\n",
        "    buffer.seek(0)\n",
        "    df = pd.read_csv(buffer, low_memory=False, skiprows=1)\n",
        "    return df\n",
        "\n",
        "@time_this\n",
        "def pandas_import(file_path):\n",
        "  df = pd.read_excel(file_path)\n",
        "  return df"
      ],
      "metadata": {
        "id": "Lz-JyysMDvIV"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fast_df = fast_import('metabolomics_data.xlsx')\n",
        "pandas_df = pandas_import('metabolomics_data.xlsx')"
      ],
      "metadata": {
        "id": "uIcKCg3PFDND",
        "outputId": "723d82a1-258c-4ee3-beb2-645fa5c867e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running fast_import took 347.360885367 seconds.\n",
            "Running pandas_import took 872.374947258 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 4: metabolomics data"
      ],
      "metadata": {
        "id": "DJVuSrn9Dq1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using vectorized operations\n",
        "Numpy carries out most data processing operations using highly optimized pre-compiled code that is orders of magnitude faster than anything Python can natively accomplish. The optimizations include single instruction, multiple data (SIMD) steps in which a single operation that would normally be applied serially to values in an array can be applied simultaniously to blocks of 4-8 values. When doing mathematical operations it is therefore best to let numpy handle as much of the processing as possible. This means that we should avoid writing explicit loops or accessing individual values within numpy arrays as much as possible and instead perform operations on whole arrays at once using numpy functions.\n",
        "\n",
        "This strategy holds true for other packages like Pandas, Scipy, and Scikit-Learn as well because Numpy provides most of the data processing functionality for these packages.  "
      ],
      "metadata": {
        "id": "yjU_X23DDUpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 5: normalization\n",
        "Proteomic normalization and imputation are important to minimize non-biological variation and ensure reliability of your dataset. Normalization is used to minimize variation coming from factors such as sample preparation inconsistencies, injection volume errors, and fluctuations in instrument sensitivity, which can happen with extended use (i.e. sensitivity drift with instrument cleanliness, or column age). By accounting for these variations, normalization allows for the comparison of biological variances rather than technical ones. Following normalization, imputation fills missing values with low abundance numbers that mimic the instrument's noise floor, allowing for statistical testing across conditions. Both normalization and imputation create a baseline from which conditions can be compared in a more robust way.\n"
      ],
      "metadata": {
        "id": "PCnOKINlDvm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The data processing produces one file per condition that we read into a list\n",
        "#of dataframes\n",
        "prot_files = [f for f in os.listdir() if f.endswith('Proteins.txt')]\n",
        "dataframes = [pd.read_csv(f, sep = '\\t') for f in prot_files]\n",
        "key_cols = ['Accession','Description','MW [kDa]','# AAs']\n",
        "#We need to merge these data into a single dataframe for further processing\n",
        "proteins = dataframes.pop()\n",
        "for df in dataframes:\n",
        "  #on = key_cols is the list of columns that we use as a key for which rows to merge\n",
        "  #how = 'outer' means we use the union of these keys\n",
        "  proteins = proteins.merge(df, how = 'outer', on = key_cols)\n",
        "#we want a list of the columns containing protein abundances for processing\n",
        "abundance_cols = [c for c in proteins.columns if c.startswith('Abundance')]\n",
        "#For this example we only want to work with the key and abundance columns\n",
        "#so we get rid of all the other data in this dataframe.\n",
        "#The * operator unpacks an interable so [1,*[2,3,4]] becomes [1,2,3,4].\n",
        "proteins = proteins[[*key_cols, *abundance_cols]]"
      ],
      "metadata": {
        "id": "QGP0UvJnUORv"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng(1)\n",
        "\n",
        "@time_this\n",
        "def vector_norm_impute(values):\n",
        "  #processing is done in the log space because intensity is log-normally\n",
        "  #distributed across proteins\n",
        "  values = np.log(values)\n",
        "  #we calculated the mean and standard deviation of all values in all samples\n",
        "  gmean = np.nanmean(values)\n",
        "  gstd = np.nanstd(values)\n",
        "  #intensities are z-scored by column (axis = 0), which corresponds to files\n",
        "  values = zscore(values, axis = 0, nan_policy = 'omit')\n",
        "  #we un-do the z-scoring with the grand mean and standard deviation so that\n",
        "  #all files now have the same log-mean and log-standard deviation\n",
        "  values = (values*gstd)+gmean\n",
        "  #we impute by taking draws from a normal distribution centered 1.8 standard\n",
        "  #deviations below the mean to simulate the correlation between missingness and\n",
        "  #intensity\n",
        "  loc = gmean - (1.8*gstd)\n",
        "  scale = gstd*0.3\n",
        "  #The strategy here is to generate a random number for each value in our data\n",
        "  #and then only transfer random values to our data at the missing value\n",
        "  #positions. We do generate significantly more random values than necessary\n",
        "  #this way but the vectorized approach is so much faster than repeated calls to\n",
        "  #rng that it is still the preferred approach\n",
        "  imputed_values = rng.normal(loc, scale, values.shape)\n",
        "  mask = np.isnan(values) #an array of true/false for if a value is missing\n",
        "  values[mask] = imputed_values[mask] #only missing values are replaced\n",
        "  #the data are taken back out of log space for downstream processing\n",
        "  values = np.exp(values)\n",
        "  return values\n",
        "\n",
        "@time_this\n",
        "def loop_norm_impute(values):\n",
        "  #this is how you do a nested loop within a list comprehension\n",
        "  logvalues = [np.log(value) for sample in values for value in sample]\n",
        "  #as above we pre-calculate variables that depend on all values\n",
        "  gmean = np.nanmean(logvalues)\n",
        "  gstd = np.nanstd(logvalues)\n",
        "  loc = gmean - (1.8*gstd)\n",
        "  scale = gstd*0.3\n",
        "  #in this verison we construct a list of lists\n",
        "  new_values = []\n",
        "  for i in range(values.shape[1]):\n",
        "    sample = values[:,i]\n",
        "    new_sample = []\n",
        "    #in order to do the normalization elementwise we have to precalculate\n",
        "    #sample-specific values\n",
        "    smean = np.nanmean(sample)\n",
        "    sstd = np.nanstd(sample)\n",
        "    for value in sample:\n",
        "      #all the processing from here out is the same but done on one element\n",
        "      #at a time\n",
        "      if np.isnan(value):\n",
        "        new_sample.append(rng.normal(loc, scale))\n",
        "      else:\n",
        "        value = np.log(value)\n",
        "        zscore = (value-smean)/sstd\n",
        "        value = (zscore*gstd)+gmean\n",
        "        value = np.exp(value)\n",
        "        new_sample.append(value)\n",
        "    new_values.append(new_sample)\n",
        "  #the .T is a transpose function which swaps rows for columns\n",
        "  return np.array(new_values).T\n"
      ],
      "metadata": {
        "id": "t_G6FHmBHp5h"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec_proteins = proteins.copy()\n",
        "vec_proteins[abundance_cols] = vector_norm_impute(vec_proteins[abundance_cols].to_numpy())\n",
        "loop_proteins = proteins.copy()\n",
        "loop_proteins[abundance_cols] = loop_norm_impute(loop_proteins[abundance_cols].to_numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwJ0saPEp_ip",
        "outputId": "86a4c208-6291-4e18-f0e3-1710b065e21d"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running vector_norm_impute took 0.013887138999990611 seconds.\n",
            "Running loop_norm_impute took 0.45028108699997915 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Building and accessing dataframes"
      ],
      "metadata": {
        "id": "xhk_IqTyEHN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 6: protein summary statistics"
      ],
      "metadata": {
        "id": "Vq4hmcioEt4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7BoJJKv8EbLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@time_this\n",
        "def numpy_stats(df):\n",
        "  df['mean'] = np.nanmean(df[abundance_cols], axis = 1)\n",
        "  df['std'] = np.nanstd(df[abundance_cols], axis = 1)\n",
        "  return df\n",
        "\n",
        "@time_this\n",
        "def listcomp_stats(df):\n",
        "  abundance_iter = list(zip(*[df[c] for c in abundance_cols]))\n",
        "  df['mean'] = [np.nanmean(row) for row in abundance_iter]\n",
        "  df['std'] = [np.nanstd(row) for row in abundance_iter]\n",
        "  return df\n",
        "\n",
        "@time_this\n",
        "def iterrows_stats(df):\n",
        "  means = []\n",
        "  stds = []\n",
        "  for i,row in df[abundance_cols].iterrows():\n",
        "    means.append(np.nanmean(row))\n",
        "    stds.append(np.nanstd(row))\n",
        "  df['mean'] = means\n",
        "  df['std'] = stds\n",
        "  return df\n",
        "\n",
        "@time_this\n",
        "def at_stats(df):\n",
        "  df['mean'] = ''\n",
        "  df['std'] = ''\n",
        "  for i,row in df[abundance_cols].iterrows():\n",
        "    df.at[i,'mean'] = np.nanmean(row)\n",
        "    df.at[i,'std'] = np.nanstd(row)\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "E16UWt2y2TI7"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numpy_df = numpy_stats(proteins.copy())\n",
        "listcomp_df = listcomp_stats(proteins.copy())\n",
        "iterrows_df = iterrows_stats(proteins.copy())\n",
        "at_df = at_stats(proteins.copy())"
      ],
      "metadata": {
        "id": "RqQlUiLb5GJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@time_this\n",
        "def melt(df):\n",
        "  df = df.melt(id_vars = key_cols, value_vars = abundance_cols)\n",
        "  return df\n",
        "\n",
        "@time_this\n",
        "def loop_melt(df):\n",
        "  dfs = [ ]\n",
        "  for a in abundance_cols:\n",
        "    tmp_df = df[[*key_cols,a]].copy()\n",
        "    tmp_df.columns = [*key_cols,'Abundnace']\n",
        "    tmp_df['sample'] = [a]*df.shape[0]\n",
        "    dfs.append(tmp_df)\n",
        "  df = pd.concat(dfs)\n",
        "  return df\n",
        "\n",
        "@time_this\n",
        "def accumulator_melt(df):\n",
        "  new_df = pd.DataFrame()\n",
        "  for a in abundance_cols:\n",
        "    tmp_df = df[[*key_cols,a]].copy()\n",
        "    tmp_df.columns = [*key_cols,'Abundnace']\n",
        "    tmp_df['sample'] = [a]*df.shape[0]\n",
        "    new_df = pd.concat([new_df, tmp_df])\n",
        "  return new_df"
      ],
      "metadata": {
        "id": "xfRuVlONvWs0"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "melt_df = melt(proteins.copy())\n",
        "print(melt_df.shape)\n",
        "loop_melt_df = loop_melt(proteins.copy())\n",
        "print(loop_melt_df.shape)\n",
        "accum_melt_df = accumulator_melt(proteins.copy())\n",
        "print(accum_melt_df.shape)\n"
      ],
      "metadata": {
        "id": "cLwdva2izOF4",
        "outputId": "bec048da-6e8e-4143-fee1-78b20ed6f75b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running melt took 0.02813604199999986 seconds.\n",
            "(108306, 6)\n",
            "Running loop_melt took 0.04736300299998675 seconds.\n",
            "(108306, 6)\n",
            "Running accumulator_melt took 0.06905706599999917 seconds.\n",
            "(108306, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Black Box Optimizers"
      ],
      "metadata": {
        "id": "Hn6uV3phH4tc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 7: isotopic packet fitting"
      ],
      "metadata": {
        "id": "2KGHUvp2Jpxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multiprocessing"
      ],
      "metadata": {
        "id": "vcegauIsICTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Numba\n",
        "The standard implementation of python is interpreted. This means that when python code is executed a program called the interpreter goes through the code line-by-line and, by way of an intermediate representation, runs just that line. This has the advantage of flexability but it requires sigificant overhead and prevents the interpreter from analyzing and optimizing your code. This is in contrast with compiled languages where all of the code is read, analyzed, optimized, and turned into machine code that your CPU can understand before anything is run.\n",
        "\n",
        "Numba gives us a stepping stone between these two strategies. It is a just-in-time compiler for python funcitons. This allows us to take performance intensive funcitons and compile them into optimized machine code at runtime. For performance reasons not all python features are available within numba but if your function can be written with a combination of base python and numpy then it can proabably be optimized with numba.\n",
        "\n",
        "Another advantage of numba is that the compiled code is not subject to the GIL so calculations can be written to run in parallel.  "
      ],
      "metadata": {
        "id": "bL1vH6LTJyZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 8: custom correlation matrix"
      ],
      "metadata": {
        "id": "hSqT96OIJ2_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###multiprocessing.Pool"
      ],
      "metadata": {
        "id": "nFxCOupHKMyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 9: bootstrapping\n"
      ],
      "metadata": {
        "id": "GTV8TwR6NLft"
      }
    }
  ]
}